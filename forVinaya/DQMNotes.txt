Adding onto Vinaya's notes and scripts from 
https://twiki.cern.ch/twiki/bin/view/CMS/TauTriggerOfflineDQM
which he originally modeled after the general DQM twiki 

Getting the release and necessary files is straightforward except for a few typos (missing spaces)
The linked workflow is compatible with 12_3_0 and L1_v1_0_0 as well

One additional correction after those repos are downloaded:
go to the file
DQMOffline/Trigger/python/HLTTauDQMOffline_cfi.py
and make the following modification to TauRefProducer

TauRefProducer = cms.EDProducer("HLTTauRefProducer",

                    PFTaus = cms.untracked.PSet(
                            PFTauDiscriminatorContainers  = cms.untracked.VInputTag(),
                            PFTauDiscriminatorContainerWPs  = cms.untracked.vstring(),
                            PFTauDiscriminators = cms.untracked.VInputTag(
                                     cms.InputTag("hpsPFTauDiscriminationByDeadECALElectronRejection", ""),
                                     cms.InputTag("hpsPFTauDiscriminationByDecayModeFinding", ""),
                                     cms.InputTag("hpsPFTauDiscriminationByDecayModeFindingNewDMs", ""),
                                     cms.InputTag("hpsPFTauDiscriminationByDecayModeFindingOldDMs", "")
                                    #cms.InputTag("hpsPFTauDiscriminationByDecayModeFinding"),
                                    #cms.InputTag("hpsPFTauDiscriminationByDecayModeFinding", ""),
                                    #cms.InputTag("hpsPFTauDiscriminationByLooseCombinedIsolationDBSumPtCorr3Hits", ""),
                                    #cms.InputTag("hpsPFTauDiscriminationByLooseMuonRejection3", ""),
                                    #cms.InputTag("hpsPFTauDiscriminationByMVA6TightElectronRejection", "")
                            ),

After that's setup, you'll make a menu with the command
hltGetConfiguration /dev/CMSSW_12_3_0/GRun/V52 --cff --offline \
--globaltag auto:phase1_2021_realistic --mc --unprescale --eras Run3 \
--setup /dev/CMSSW_12_3_0/GRun/V52 --type "GRun"  \
--l1-emulator FullMC --l1Xml L1Menu _Collisions2022_v0_1_6.xml > HLT_User_cff.py

Where you'll update the menu and the --setup argument to be the same menu, and use the latest supported L1

In the generated file, you'll modify this line
fragment.load("setup_dev_CMSSW_12_3_0_GRun_V52_cff")
to
fragment.load("HLTrigger.Configuration.setup_dev_CMSSW_12_3_0_GRun_V52_cff")

Then you'll copy these files to some directories
cp setup_dev_CMSSW_12_3_0_GRun_V52_cff.py HLTrigger/Configuration/python/
cp HLT_User_cff.py HLTrigger/Configuration/python/

scram b -j 24

Everytime you modify your conf you have to re-copy and scram everything again (sucks)

Next, clone this repo
git clone https://github.com/vmuralee/DQMutilities.git
you'll need the files cmsCondorReHLT.py, cmsCondorMCDQM.py, and PlotDQM.py

We need to generate a file to submit along with all of our input samples, so we use the following cmsDriver command
(can run interactively on one file by removing --no_exec argument)
cmsDriver.py step0 -s HLT:User --conditions auto:phase1_2021_realistic \
--era Run3 --mc --eventcontent FEVTDEBUGHLT --hltProcess reHLT \
--filein /store/mc/Run3Summer21DRPremix/VBFHToTauTau_M125_TuneCP5_14TeV-powheg-pythia8/GEN-SIM-DIGI-RAW/120X_mcRun3_2021_realistic_v6-v2/30000/e788d346-c093-47de-a241-8ac345e13f32.root \
--inputCommands="keep *,drop Run3Scouting*_hltScouting*Packer__HLT"  \
--python_filename=rerun_hlt_cfg.py --processName=reHLT -n -1 --no_exec

Note that we're NOT running on a RelVal sample, so no validation steps are necessary

Before submitting condor, re-voms if you haven't already, and copy your proxy to your working directory (the one you'll send the condor command from)
voms-proxy-init -voms cms -rfc -valid 48:00
cp /tmp/[proxy] ./

Next we need the list of files that make up our dataset
dasgoclient --query 'file dataset=/VBFHToTauTau_M125_TuneCP5_14TeV-powheg-pythia8/Run3Summer21DRPremix-120X_mcRun3_2021_realistic_v6-v2/GEN-SIM-DIGI-RAW' > list_VBF

Each successful output file is 4GB and the default AFS space is 1TB (1000 GB), so remove lines from the generated list until you have about 100 files

Now we make jobs with condor with the following command
python3 cmsCondorReHLT.py rerun_hlt_cfg.py /afs/cern.ch/user/b/ballmond/public/CMSSW_12_3_0_pre6/src /eos/user/b/ballmond/VBFNtuples/ list_VBF_Trimmed -n 1 -q workday -p /public/CMSSW_12_3_0_pre6/src/HLTrigger/Configuration/test/x509up_u134427

And then submit them with
. sub_total.jobb

After they're submitted, you can check their status by typing the following into the command line
condor_q

You can also monitor logs via
vi Jobs/Job_0/hlt.log
as well as any relevant errors via
vi Jobs/Job_0/hlt.stderr

Many of these jobs will fail due to nebulous errors unrelated to your conf. Simply run another job with a different set of input files until you have about 20 good output files

Once these jobs are complete, we make another file and submit another round of jobs to condor. 
The file to make is step1_RAW2DIGI_L1Reco_RECO_DQM.py
Which you do with the following command
cmsDriver.py step1 -s RAW2DIGI,L1Reco,RECO,DQM --eventcontent DQM --datatier DQMIO --conditions auto:phase1_2021_realistic --era Run3 --geometry DB:Extended --process reRECO --filein file:/eos/user/b/ballmond/VBFNtuples/step0_HLT_10.root -n -1 --mc --hltProcess reHLT

The argument --hltProcess reHLT is absolutely necessary
The argument --filein file:/eos/user/... should point to a completed output file from your previously submitted condor command

We need to make a list of finished files from the previous condor command. You can do this with the "find" command 
find /eos/user/b/ballmond/VBFNtuples/ > list_step0_HLT.txt  (change to your output directory from last condor job)
Remember to delete the top line since it's the home directory of the files and won't be a good argument for the condor job.

Now we make jobs with condor with the following command (make your output directory different than before)
python3 cmsCondorMCDQM.py step1_RAW2DIGI_L1Reco_RECO_DQM.py /afs/cern.ch/user/b/ballmond/public/CMSSW_12_3_0/src/ /eos/user/b/ballmond/VBFNtuplesDQM/ list_step0_HLT.txt -n 1 -q workday

I'm not sure why we don't need a proxy this time.
You can monitor the same way as before. Once those jobs are done, we need to run this command on a completed file.

cmsDriver.py step2 -s HARVESTING:dqmHarvesting --harvesting AtRunEnd  --conditions auto:phase1_2021_realistic --era Run3 --geometry DB:Extended --mc --scenario pp --filein file:/eos/user/b/ballmond/VBFNtuplesDQM/step1_RAW2DIGI_L1Reco_RECO_DQM_0.root --filetype DQM -n -1

Like before, crtl+c this command once a few events have been run. We'll now modify the input files for this command from
process.source = cms.Source("DQMRootSource",
    fileNames = cms.untracked.vstring( 'file:/eos/user/b/ballmond/VBFNtuplesDQM/step1_RAW2DIGI_L1Reco_RECO_DQM_0.root')
to
process.source = cms.Source("DQMRootSource",
    fileNames = cms.untracked.vstring(
      'file:/eos/user/b/ballmond/VBFNtuplesDQM/step1_RAW2DIGI_L1Reco_RECO_DQM_0.root',
      'file:/eos/user/b/ballmond/VBFNtuplesDQM/step1_RAW2DIGI_L1Reco_RECO_DQM_1.root',
      'file:/eos/user/b/ballmond/VBFNtuplesDQM/step1_RAW2DIGI_L1Reco_RECO_DQM_10.root' #+however many filenames you have
    )

This was easiest to do by copying and pasting the file locations from a "find" command into the python file and then using vi substitutions to add the necessary 'file: and ', around the filenames. Remember your last filename will not have a trailing comma

Now run this new file.
cmsRun step2_HARVESTING.py

If successful, this produces the DQM file 
DQM_V0001_R000000001__Global__CMSSW_X_Y_Z__RECO.root

This file can be opened normally 
root -l DQM_V0001_R000000001__Global__CMSSW_X_Y_Z__RECO.root
and then you can navigate to the plots with 
_file0->cd("DQMData/Run 1/HLT/Run summary/TAU/PFTaus/")

You should see your paths if they had "PFTau" in their name somewhere.

Nominally this is what goes into the plotting script. Remember to X forward your ssh so you can see generated plots on lxplus. 

To get the plots you can use TBrowser and take a screenshot after navigating to the file with the gui
root -l DQM_filename.root
new TBrowser

This is incredibly slow, so you can also add an entry to PlotDQM.py for your specific paths IF you know the names of the eff files they automatically generate
(i had to find the files using the TBrowser, then I added an entry, and was able to use PlotDQM.py to make a directory of images)
By adding these lines 
                "VBF-tau":[
                            "HLT_IsoMu24_eta2p1_MediumDeepTauIsoPFTauHPS45_L2NN_eta2p1_CrossL1",
                            ["L2TrigTauEtEff","L2TrigTauHighEtEff","L3TrigMuonEtEff","L3TrigTauHighEtEff"]
                         ]

Another thing you may do is add modules for automatically generating eff files for TagAndProbe. Add the following lines to the end of the file
DQMOffline/Trigger/python/HLTTauDQMOffline_cfi.py

        cms.untracked.PSet(
            name        = cms.string('UpperControlTau45'),
            xvariable   = cms.string('Tau'),
            nPtBins     = cms.int32(20),
            ptmin       = cms.double(0.),
            ptmax       = cms.double(200.),
            nEtaBins    = cms.int32(20),
            etamin      = cms.double(-2.5),
            etamax      = cms.double(2.5),
            nPhiBins    = cms.int32(20),
            phimin      = cms.double(-3.15),
            phimax      = cms.double(3.15),
            numerator   = TriggerSelectionParameters(cms.vstring('HLT_IsoMu24_eta2p1_MediumDeepTauIsoPFTauHPS45_L2NN_eta2p1_CrossL1_v*')),
            denominator = TriggerSelectionParameters(cms.vstring('HLT_IsoMu24_v*'))
        ),
        cms.untracked.PSet(
            name        = cms.string('LowerControlTau20'),
            xvariable   = cms.string('Tau'),
            nPtBins     = cms.int32(20),
            ptmin       = cms.double(0.),
            ptmax       = cms.double(200.),
            nEtaBins    = cms.int32(20),
            etamin      = cms.double(-2.5),
            etamax      = cms.double(2.5),
            nPhiBins    = cms.int32(20),
            phimin      = cms.double(-3.15),
            phimax      = cms.double(3.15),
            numerator   = TriggerSelectionParameters(cms.vstring('HLT_IsoMu24_eta2p1_MediumDeepTauIsoPFTauHPS20_eta2p1_SingleL1_v*')),
            denominator = TriggerSelectionParameters(cms.vstring('HLT_IsoMu24_v*'))
        )

make sure to recompile 
next rerun the harvesting step (this produces the graphs, and the file we just modified has instructions for which graphs to produce and save)

You can view the graphs the same as before using a TBrowser (incredibly slow, but it does work)
